{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958ad24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas>=1.3.0\n",
    "numpy>=1.21.0\n",
    "requests>=2.26.0\n",
    "sqlalchemy>=1.4.0\n",
    "pydantic-settings>=2.0.0\n",
    "pydantic>=2.0.0\n",
    "python-dotenv>=0.19.0\n",
    "pandera>=0.6.0\n",
    "pyarrow>=6.0.0\n",
    "# For the forecasting notebook\n",
    "prophet\n",
    "statsmodels\n",
    "# For database connection\n",
    "pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34a2a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\"\"\"\n",
    "Configuration management with environment variables\n",
    "\"\"\"\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field\n",
    "\n",
    "class BaseConfig(BaseSettings):\n",
    "    \"\"\"Defines shared configuration for all settings classes.\"\"\"\n",
    "    class Config:\n",
    "        env_file = '.env'\n",
    "        env_file_encoding = 'utf-8'\n",
    "        extra = 'ignore'\n",
    "\n",
    "class DatabaseSettings(BaseConfig):\n",
    "    \"\"\"Database connection settings.\"\"\"\n",
    "    user: str = Field(..., alias='DB_USER')\n",
    "    password: str = Field(..., alias='DB_PASSWORD')\n",
    "    host: str = Field('localhost', alias='DB_HOST')\n",
    "    port: int = Field(3306, alias='DB_PORT')\n",
    "    database: str = Field(..., alias='DB_NAME')\n",
    "    pool_size: int = Field(10, alias='DB_POOL_SIZE')\n",
    "    max_overflow: int = Field(5, alias='DB_MAX_OVERFLOW')\n",
    "\n",
    "    @property\n",
    "    def url(self) -> str:\n",
    "        \"\"\"Constructs the database connection URL.\"\"\"\n",
    "        return f\"mysql+pymysql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "# Initialize the config object\n",
    "db_config = DatabaseSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e249a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting db_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile db_loader.py\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "from config import db_config\n",
    "import logging\n",
    "from pandera import Check, Column, DataFrameSchema\n",
    "from pandera.errors import SchemaError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DBLoader:\n",
    "    def __init__(self):\n",
    "        self.engine = self._create_engine()\n",
    "\n",
    "    def _create_engine(self):\n",
    "        return create_engine(\n",
    "            db_config.url,\n",
    "            pool_size=db_config.pool_size,\n",
    "            max_overflow=db_config.max_overflow,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600\n",
    "        )\n",
    "\n",
    "    def validate_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates the DataFrame against a predefined schema.\"\"\"\n",
    "        schema = DataFrameSchema({\n",
    "            \"date\": Column(pd.Timestamp),\n",
    "            # FIX: Allow NAV to be zero or greater\n",
    "            \"nav\": Column(float, checks=[Check.greater_than_or_equal_to(0)]),\n",
    "            \"scheme_code\": Column(str),\n",
    "            \"scheme_name\": Column(str)\n",
    "        })\n",
    "        try:\n",
    "            schema.validate(df, lazy=True)\n",
    "            return True\n",
    "        except SchemaError as e:\n",
    "            logger.error(f\"Data validation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_to_db(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> bool:\n",
    "        if not self.validate_data(df):\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                if if_exists == 'append' and not df.empty:\n",
    "                    unique_dates = df['date'].dt.strftime('%Y-%m-%d').unique()\n",
    "                    date_list_for_sql = \", \".join([f\"'{d}'\" for d in unique_dates])\n",
    "                    \n",
    "                    delete_sql = text(f\"DELETE FROM {table_name} WHERE date IN ({date_list_for_sql})\")\n",
    "                    result = conn.execute(delete_sql)\n",
    "                    logger.info(f\"Removed {result.rowcount} existing records for dates being loaded.\")\n",
    "\n",
    "                df.to_sql(\n",
    "                    name=table_name, con=conn, if_exists=if_exists,\n",
    "                    index=False, method='multi', chunksize=1000\n",
    "                )\n",
    "                logger.info(f\"Successfully loaded {len(df)} records to table '{table_name}' with mode '{if_exists}'.\")\n",
    "                return True\n",
    "        except SQLAlchemyError as e:\n",
    "            logger.error(f\"DB Error during load to '{table_name}': {e}\", exc_info=True)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82e922ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_full_history_optimized.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_full_history_optimized.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from db_loader import DBLoader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "MAX_WORKERS = 20\n",
    "CHECKPOINT_INTERVAL_SECONDS = 300\n",
    "PROGRESS_FILE = \"_progress.pkl\"\n",
    "ALL_FUNDS_FILE = \"_all_funds.pkl\"\n",
    "BACKUP_FILE = \"final_nav_data.parquet\"\n",
    "\n",
    "# (All the fetch functions remain the same as before)\n",
    "def fetch_all_scheme_codes() -> pd.DataFrame:\n",
    "    if os.path.exists(ALL_FUNDS_FILE):\n",
    "        logging.info(\"Loading master fund list from local cache...\")\n",
    "        return pd.read_pickle(ALL_FUNDS_FILE)\n",
    "    logging.info(\"Fetching master list of all fund schemes from API...\")\n",
    "    try:\n",
    "        response = requests.get(\"https://api.mfapi.in/mf\", timeout=30)\n",
    "        response.raise_for_status()\n",
    "        funds_df = pd.DataFrame(response.json())\n",
    "        funds_df.to_pickle(ALL_FUNDS_FILE)\n",
    "        logging.info(f\"Found and cached {len(funds_df)} total schemes.\")\n",
    "        return funds_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not fetch the master fund list: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_one_fund_history(session: requests.Session, fund_info: dict) -> pd.DataFrame:\n",
    "    scheme_code = fund_info['schemeCode']\n",
    "    scheme_name = fund_info['schemeName']\n",
    "    url = f\"https://api.mfapi.in/mf/{scheme_code}\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=20)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            records = data.get(\"data\")\n",
    "            if data.get(\"status\") == \"SUCCESS\" and records:\n",
    "                df = pd.DataFrame(records)\n",
    "                df['scheme_code'] = scheme_code\n",
    "                df['scheme_name'] = scheme_name\n",
    "                return df[['date', 'scheme_code', 'scheme_name', 'nav']]\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline with all optimizations.\"\"\"\n",
    "    # Check if we have a backup file to use instead of re-downloading\n",
    "    if os.path.exists(BACKUP_FILE):\n",
    "        logging.warning(f\"--- Found existing backup file '{BACKUP_FILE}'. Loading from it instead of re-downloading. ---\")\n",
    "        final_df = pd.read_parquet(BACKUP_FILE)\n",
    "    else:\n",
    "        # (The entire download logic is here, but will be skipped if backup exists)\n",
    "        all_funds = fetch_all_scheme_codes()\n",
    "        if all_funds.empty: return\n",
    "        all_data_frames = []\n",
    "        processed_codes = set()\n",
    "\n",
    "        if os.path.exists(PROGRESS_FILE):\n",
    "            with open(PROGRESS_FILE, 'rb') as f: all_data_frames = pickle.load(f)\n",
    "            for df in all_data_frames: processed_codes.update(df['scheme_code'].unique())\n",
    "            logging.info(f\"Resuming with {len(processed_codes)} funds already downloaded.\")\n",
    "        \n",
    "        remaining_funds = all_funds[~all_funds['schemeCode'].isin(processed_codes)]\n",
    "        \n",
    "        if not remaining_funds.empty:\n",
    "            session = requests.Session()\n",
    "            adapter = requests.adapters.HTTPAdapter(pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS)\n",
    "            session.mount('https://', adapter)\n",
    "            fund_list = remaining_funds.to_dict('records')\n",
    "            last_checkpoint_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                future_to_fund = {executor.submit(fetch_one_fund_history, session, fund): fund for fund in fund_list}\n",
    "                for future in tqdm(as_completed(future_to_fund), total=len(fund_list), desc=\"Fetching Full History\"):\n",
    "                    result_df = future.result()\n",
    "                    if not result_df.empty: all_data_frames.append(result_df)\n",
    "                    if time.time() - last_checkpoint_time > CHECKPOINT_INTERVAL_SECONDS:\n",
    "                        with open(PROGRESS_FILE, 'wb') as f: pickle.dump(all_data_frames, f)\n",
    "                        tqdm.write(\"Checkpoint saved.\")\n",
    "                        last_checkpoint_time = time.time()\n",
    "        \n",
    "        with open(PROGRESS_FILE, 'wb') as f: pickle.dump(all_data_frames, f)\n",
    "        if not all_data_frames:\n",
    "            logging.error(\"CRITICAL: No data could be downloaded. Halting.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Combining all data sources and cleaning...\")\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        final_df['nav'] = pd.to_numeric(final_df['nav'], errors='coerce')\n",
    "        final_df['date'] = pd.to_datetime(final_df['date'], dayfirst=True, errors='coerce')\n",
    "        final_df.dropna(subset=['nav', 'date', 'scheme_code', 'scheme_name'], inplace=True)\n",
    "        final_df['scheme_code'] = final_df['scheme_code'].astype(str).str.strip()\n",
    "        final_df.sort_values('date', ascending=True, inplace=True)\n",
    "        final_df.drop_duplicates(subset=['date', 'scheme_code'], keep='last', inplace=True)\n",
    "        \n",
    "        logging.info(f\"--- Backup --- Saving final combined data to '{BACKUP_FILE}'...\")\n",
    "        final_df.to_parquet(BACKUP_FILE, index=False)\n",
    "\n",
    "    # --- FINAL PROCESSING ---\n",
    "    logging.info(\"Final data processing and validation...\")\n",
    "\n",
    "    # --- ADD THIS LINE TO FIX THE ERROR ---\n",
    "    final_df = final_df[final_df['nav'] >= 0].copy()\n",
    "    \n",
    "    logging.info(f\"Preparing to load {len(final_df)} valid records into the database.\")\n",
    "    loader = DBLoader()\n",
    "    success = loader.load_to_db(final_df, 'nav_data', if_exists='replace')\n",
    "\n",
    "    if success:\n",
    "        logging.info(\"âœ… Full historical database has been built successfully!\")\n",
    "        if os.path.exists(PROGRESS_FILE): os.remove(PROGRESS_FILE)\n",
    "        if os.path.exists(ALL_FUNDS_FILE): os.remove(ALL_FUNDS_FILE)\n",
    "    else:\n",
    "        logging.error(\"âŒ Database loading failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9600a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting update_daily.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile update_daily.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "from io import StringIO\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from db_loader import DBLoader\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def create_resilient_session():\n",
    "    \"\"\"\n",
    "    Creates a requests session with automatic retry logic for network reliability.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=3,                # Total number of retries\n",
    "        backoff_factor=1,       # Wait 1s, 2s, 4s between retries\n",
    "        status_forcelist=[429, 500, 502, 503, 504], # Retry on these server errors\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def fetch_latest_daily_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches the latest daily NAV data for all funds from AMFI using a resilient session.\n",
    "    \"\"\"\n",
    "    logging.info(\"Fetching latest daily data from AMFI...\")\n",
    "    amfi_url = \"https://www.amfiindia.com/spages/NAVAll.txt\"\n",
    "    session = create_resilient_session()\n",
    "\n",
    "    try:\n",
    "        response = session.get(amfi_url, timeout=30)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        # The rest of the parsing logic remains the same\n",
    "        all_lines = response.text.strip().splitlines()\n",
    "        header_index = next((i for i, line in enumerate(all_lines) if \"Scheme Name\" in line), -1)\n",
    "        if header_index == -1:\n",
    "            logging.error(\"Could not find the header row in the AMFI data.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        data_lines = [line for line in all_lines[header_index + 1:] if line.count(';') > 4]\n",
    "        if not data_lines:\n",
    "            logging.warning(\"No valid data lines found after the header.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        data_string = \"\\n\".join(data_lines)\n",
    "        df = pd.read_csv(\n",
    "            StringIO(data_string), sep=';', header=None, usecols=[0, 3, 4, 5],\n",
    "            names=['scheme_code', 'scheme_name', 'nav', 'date']\n",
    "        )\n",
    "\n",
    "        # Clean the data\n",
    "        df['nav'] = pd.to_numeric(df['nav'], errors='coerce')\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d-%b-%Y', errors='coerce')\n",
    "        df.dropna(subset=['nav', 'date', 'scheme_code'], inplace=True)\n",
    "        df = df[df['nav'] >= 0].copy() # Filter out invalid negative NAVs\n",
    "        df['scheme_code'] = df['scheme_code'].astype(str).str.strip()\n",
    "        \n",
    "        logging.info(f\"Successfully parsed {len(df)} records from AMFI.\")\n",
    "        return df\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch data from AMFI after retries: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during data parsing: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the daily update.\"\"\"\n",
    "    daily_df = fetch_latest_daily_data()\n",
    "    if daily_df.empty:\n",
    "        logging.warning(\"No daily data fetched. Exiting.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Fetched {len(daily_df)} new records. Loading to database...\")\n",
    "    loader = DBLoader()\n",
    "    # Use 'append' mode - the loader will handle duplicates\n",
    "    success = loader.load_to_db(daily_df, 'nav_data', if_exists='append')\n",
    "\n",
    "    if success:\n",
    "        logging.info(\"âœ… Daily update completed successfully.\")\n",
    "    else:\n",
    "        logging.error(\"âŒ Daily update failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32d222d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing varify_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile varify_data.py \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from config import db_config\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def view_data():\n",
    "    \"\"\"Connects to the database and displays the most recent data.\"\"\"\n",
    "    logging.info(f\"Connecting to database '{db_config.database}'...\")\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(db_config.url)\n",
    "        with engine.connect() as conn:\n",
    "            logging.info(\"âœ… Connection successful!\")\n",
    "            \n",
    "            query = text(\"SELECT * FROM nav_data ORDER BY date DESC LIMIT 10\")\n",
    "            df = pd.read_sql(query, conn)\n",
    "            \n",
    "            if df.empty:\n",
    "                logging.warning(\"Database connected, but the 'nav_data' table is empty.\")\n",
    "            else:\n",
    "                print(\"\\n--- 10 Most Recent Records in Your Database ---\")\n",
    "                print(df.to_string())\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ Failed to connect or query the database: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    view_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b57a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Analysis_Optimized.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%writefile Analysis_Optimized.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Mutual Fund Analysis & Recommendation Engine\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a three-part, in-depth analysis of mutual funds:\\n\",\n",
    "    \"1.  **Comprehensive Analysis Engine:** Calculates a full statistical profile for every fund.\\n\",\n",
    "    \"2.  **Dynamic Recommendation Engine:** Recommends the best funds based on your specific investment timeline.\\n\",\n",
    "    \"3.  **Supporting Visual Analysis:** Tools for diversification and deep-diving into individual funds.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Part 1: The Analysis Engine (The \\\"Backend\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"### ðŸ” Step 1: Setup & Memory-Safe SQL Data Loading\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Establish a robust and memory-efficient connection to the mutual fund database. The logic first filters out non-relevant or structurally unsuitable mutual funds (e.g., closed-ended, FMPs, ETFs) using a comprehensive keyword-based exclusion list.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"This filtering ensures we're analyzing only **open-ended, investable funds** meant for long-term investors and avoids skewed analysis from temporary or fixed-structure funds. Memory efficiency is critical for large-scale databases, especially in Jupyter environments.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Part 1: Setup and SQL-Only Memory-Safe Data Loading (Corrected Logic) ---\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from sqlalchemy import create_engine\\n\",\n",
    "    \"from config import db_config # Assuming config.py is in the same directory\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from scipy.cluster import hierarchy\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Setup plotting and display styles ---\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (18, 9)\\n\",\n",
    "    \"pd.set_option('display.float_format', lambda x: f'{x:.2f}')\\n\",\n",
    "    \"pd.set_option('display.width', 1000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- MEMORY-SAFE DATA LOADING FROM SQL ---\\n\",\n",
    "    \"print(\\\"--- Step 1: Connecting to SQL Server to identify unsuitable funds to exclude ---\\\")\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    engine = create_engine(db_config.url)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Query 1: Get the small, unique list of all fund schemes\\n\",\n",
    "    \"    all_funds_query = \\\"SELECT DISTINCT scheme_code, scheme_name FROM nav_data\\\"\\n\",\n",
    "    \"    all_funds_df = pd.read_sql(all_funds_query, engine)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Use the comprehensive keyword list to identify unsuitable funds\\n\",\n",
    "    \"    filter_keywords = [\\n\",\n",
    "    \"    # Closed-Ended / Fixed Term / Un-investable\\n\",\n",
    "    \"    'FMP', 'FIXED MATURITY', 'FIXED TERM', 'SERIES', 'INTERVAL FUND', \\n\",\n",
    "    \"    'CAPITAL PROTECTION', 'CLOSED ENDED', 'CLOSE ENDED', 'CLOSE-ENDED', \\n\",\n",
    "    \"    'CAP PROTECTION', 'LIMITED OFFER', 'NFO', 'MATURITY', 'TARGET MATURITY',\\n\",\n",
    "    \"    'SEGREGATED PORTFOLIO', 'LOCK-IN', 'LIMITED PERIOD',\\n\",\n",
    "    \"    # Cash-Equivalent / Extreme Low-Risk Debt (not for growth comparison)\\n\",\n",
    "    \"    'OVERNIGHT', 'LIQUID', 'ULTRA SHORT', 'ULTRA-SHORT', 'MONEY MARKET', \\n\",\n",
    "    \"    'GILT', 'ARBITRAGE', 'SHORT DURATION', 'LOW DURATION', 'CORPORATE BOND',\\n\",\n",
    "    \"    'CREDIT RISK', 'DYNAMIC BOND', 'BANKING AND PSU',\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Other Structures\\n\",\n",
    "    \"    'ETF'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    keyword_pattern = '|'.join(filter_keywords)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    all_funds_df['scheme_name'] = all_funds_df['scheme_name'].astype(str)\\n\",\n",
    "    \"    unsuitable_funds = all_funds_df[all_funds_df['scheme_name'].str.contains(keyword_pattern, case=False, na=False)]\\n\",\n",
    "    \"    schemes_to_exclude = unsuitable_funds['scheme_code'].tolist()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Identified {len(schemes_to_exclude)} unsuitable schemes (FMPs, Fixed Term, etc.) to exclude from loading.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Step 2: Build a specific SQL query to load ONLY the required historical data ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 2: Loading historical data for suitable, open-ended funds from SQL ---\\\")\\n\",\n",
    "    \"    if schemes_to_exclude:\\n\",\n",
    "    \"        # Create a string of scheme codes for the SQL 'NOT IN' clause\\n\",\n",
    "    \"        exclude_list_str = \\\", \\\".join([f\\\"'{code}'\\\" for code in schemes_to_exclude])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Query 2: The smart query that prevents MemoryErrors by excluding unsuitable funds\\n\",\n",
    "    \"        data_query = f\\\"SELECT * FROM nav_data WHERE scheme_code NOT IN ({exclude_list_str})\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        df = pd.read_sql(data_query, engine, parse_dates=['date'])\\n\",\n",
    "    \"        print(f\\\"Successfully loaded a manageable subset of {len(df)} records.\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # If no funds are excluded, load everything (might cause MemoryError on large DBs)\\n\",\n",
    "    \"        print(\\\"No unsuitable funds found to exclude. Loading the entire dataset.\\\")\\n\",\n",
    "    \"        df = pd.read_sql(\\\"SELECT * FROM nav_data\\\", engine, parse_dates=['date'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"An error occurred while connecting to the database: {e}\\\")\\n\",\n",
    "    \"    df = pd.DataFrame(columns=['scheme_code', 'scheme_name', 'date', 'nav'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ§  Step 2: Comprehensive Statistical Engine\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Build a complete historical and statistical profile for each mutual fund. We calculate historical NAV returns (1Y, 3Y, 5Y, 10Y), CAGR, fund age, and since-inception returns.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Innovations:** \\n\",\n",
    "    \"- A memory-efficient `.loc` and `.get_indexer()` based custom function is used to fetch historical NAVsâ€”avoiding `merge_asof` which is error-prone on large data.  \\n\",\n",
    "    \"- Returns are normalized to **CAGR** format to make funds across different timeframes comparable.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"These metrics are essential for investors to assess consistency, longevity, and performance expectations.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ“Š Step 3: Risk and Return Statistical Profile\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Add a quantitative risk layer to the fund analysis by calculating:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Annual Expected Return**\\n\",\n",
    "    \"- **Annualized Volatility**\\n\",\n",
    "    \"- **Sharpe Ratio**\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"This moves us beyond raw returns into **risk-adjusted performance**â€”critical for understanding whether high returns came with disproportionately high volatility. Sharpe Ratio, in particular, helps to compare \\\"return per unit of risk\\\".\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ“‹ Step 4: Curated Display Table\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Produce a **final investor-friendly table** that summarizes all key statistics and ranks funds by long-term performance (5Y returns).\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"This acts as the \\\"backend summary\\\" that feeds into both the visual layer and the recommendation engine. Itâ€™s the first high-level insight view for fund analysts or end-users.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Part 2: The Main Analysis Engine (New, Robust Method) ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not df.empty:\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 3: Calculating comprehensive metrics for all funds ---\\\")\\n\",\n",
    "    \"    df['daily_return'] = df.groupby('scheme_code')['nav'].pct_change()\\n\",\n",
    "    \"    df_sorted = df.sort_values(['scheme_code', 'date'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Prepare clean, sorted base dataframes\\n\",\n",
    "    \"    first_navs = df_sorted.loc[df_sorted.groupby('scheme_code')['date'].idxmin()]\\n\",\n",
    "    \"    latest_navs = df_sorted.loc[df_sorted.groupby('scheme_code')['date'].idxmax()]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    master_analysis_df = latest_navs[['scheme_code', 'scheme_name', 'date', 'nav']].rename(columns={'date': 'latest_date', 'nav': 'latest_nav'})\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df.merge(first_navs[['scheme_code', 'date', 'nav']], on='scheme_code', how='inner')\\n\",\n",
    "    \"    master_analysis_df.rename(columns={'date': 'inception_date', 'nav': 'inception_nav'}, inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Activity Filter (remove funds that haven't reported recently)\\n\",\n",
    "    \"    last_valid_date = pd.Timestamp.now() - pd.DateOffset(days=30)\\n\",\n",
    "    \"    activity_mask = master_analysis_df['latest_date'] >= last_valid_date\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df[activity_mask].copy()\\n\",\n",
    "    \"    print(f\\\"Final number of active, open-ended funds for analysis: {len(master_analysis_df)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- NEW METHOD: Using an indexed lookup to avoid merge_asof ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 4: Calculating all historical returns (new robust method) ---\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Create a fast, indexed version of the historical data for lookups\\n\",\n",
    "    \"    df_indexed = df_sorted.set_index(['scheme_code', 'date'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 2. Define a function to get the nearest NAV for a given date\\n\",\n",
    "    \"    def get_historical_nav(scheme_code, target_date, indexed_df):\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Get all dates for the specific fund\\n\",\n",
    "    \"            fund_dates = indexed_df.loc[scheme_code].index\\n\",\n",
    "    \"            # Find the index position of the nearest date\\n\",\n",
    "    \"            nearest_index_pos = fund_dates.get_indexer([target_date], method='nearest')[0]\\n\",\n",
    "    \"            # Get the actual nearest date from that position\\n\",\n",
    "    \"            nearest_date = fund_dates[nearest_index_pos]\\n\",\n",
    "    \"            # Return the NAV for that specific scheme and date\\n\",\n",
    "    \"            return indexed_df.loc[(scheme_code, nearest_date), 'nav']\\n\",\n",
    "    \"        except KeyError:\\n\",\n",
    "    \"            # This handles cases where a fund might not be in the indexed data\\n\",\n",
    "    \"            return np.nan\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 3. Apply this function for each period\\n\",\n",
    "    \"    for years in [1, 3, 5, 10]:\\n\",\n",
    "    \"        target_date_col = f'date_{years}y_ago'\\n\",\n",
    "    \"        nav_col = f'nav_{years}y_ago'\\n\",\n",
    "    \"        master_analysis_df[target_date_col] = master_analysis_df['latest_date'] - pd.DateOffset(years=years)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        master_analysis_df[nav_col] = master_analysis_df.apply(\\n\",\n",
    "    \"            lambda row: get_historical_nav(row['scheme_code'], row[target_date_col], df_indexed),\\n\",\n",
    "    \"            axis=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    # --- Calculate CAGR using the new columns ---\\n\",\n",
    "    \"    def calculate_cagr(start_value, end_value, years):\\n\",\n",
    "    \"        if pd.isna(start_value) or pd.isna(end_value) or start_value <= 0 or years <= 0.25:\\n\",\n",
    "    \"            return np.nan\\n\",\n",
    "    \"        return ((end_value / start_value) ** (1 / years) - 1) * 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for p in [1, 3, 5, 10]:\\n\",\n",
    "    \"        master_analysis_df[f'{p}Y Return (%)'] = master_analysis_df.apply(\\n\",\n",
    "    \"            lambda row: calculate_cagr(row[f'nav_{p}y_ago'], row['latest_nav'], p),\\n\",\n",
    "    \"            axis=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    master_analysis_df['Fund Age (Yrs)'] = (master_analysis_df['latest_date'] - master_analysis_df['inception_date']).dt.days / 365.25\\n\",\n",
    "    \"    master_analysis_df['Since Inception Return (%)'] = master_analysis_df.apply(\\n\",\n",
    "    \"        lambda row: calculate_cagr(row['inception_nav'], row['latest_nav'], row['Fund Age (Yrs)']),\\n\",\n",
    "    \"        axis=1\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Statistical Profile (Risk and Expectation) ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 5: Calculating statistical profile ---\\\")\\n\",\n",
    "    \"    daily_returns_grouped = df.groupby('scheme_code')['daily_return']\\n\",\n",
    "    \"    mean_returns = daily_returns_grouped.mean().to_frame(name='mean')\\n\",\n",
    "    \"    std_returns = daily_returns_grouped.std().to_frame(name='std')\\n\",\n",
    "    \"    stats_df = mean_returns.join(std_returns).reset_index()\\n\",\n",
    "    \"    stats_df['Expected Annual Return (%)'] = stats_df['mean'] * 252 * 100\\n\",\n",
    "    \"    stats_df['Annualized Volatility (%)'] = stats_df['std'] * np.sqrt(252) * 100\\n\",\n",
    "    \"    risk_free_rate_daily = (1.04 ** (1/252)) - 1\\n\",\n",
    "    \"    stats_df['Sharpe Ratio'] = (stats_df['mean'] - risk_free_rate_daily) / stats_df['std'] * np.sqrt(252)\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df.merge(stats_df[['scheme_code', 'Expected Annual Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio']], on='scheme_code', how='left')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Final Display Table ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 6: Generating Final Report ---\\\")\\n\",\n",
    "    \"    display_cols = [\\n\",\n",
    "    \"        'scheme_name', 'Fund Age (Yrs)', 'Since Inception Return (%)', '10Y Return (%)', '5Y Return (%)',\\n\",\n",
    "    \"        '3Y Return (%)', '1Y Return (%)', 'Expected Annual Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    display_df = master_analysis_df[display_cols].sort_values('5Y Return (%)', ascending=False).copy()\\n\",\n",
    "    \"    for col in ['10Y Return (%)', '5Y Return (%)', '3Y Return (%)', '1Y Return (%)', 'Since Inception Return (%)']:\\n\",\n",
    "    \"        display_df[col] = display_df[col].apply(lambda x: f\\\"{x:.2f}\\\" if pd.notna(x) else '-')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\\"\\\\n--- Master Analysis Table: Curated for Investors ---\\\")\\n\",\n",
    "    \"    display(display_df.head(20))\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nAnalysis skipped because no data was loaded in Part 1.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ§  Part 2: The Recommendation Engine (Frontend)\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Based on the user's investment horizon and risk tolerance, score and rank mutual funds using a **custom suitability algorithm**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Logic:**\\n\",\n",
    "    \"- Risk-normalized scoring weights based on user profile.\\n\",\n",
    "    \"- Funds must match or exceed the desired investment duration.\\n\",\n",
    "    \"- Split results into **Core (diversified)** and **Specialized (thematic)**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"This system generates **personalized, goal-aligned fund recommendations** while distinguishing between diversified vs. sector-specific options.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### --- Set Your Investment Goal Below ---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Part 3: The Recommendation Engine (with Core vs. Specialized Funds) ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ======================================================\\n\",\n",
    "    \"# ===> SET YOUR INVESTMENT GOALS HERE <===\\n\",\n",
    "    \"INVESTMENT_HORIZON_YEARS = 1\\n\",\n",
    "    \"RISK_TOLERANCE = 'High'  # Options: 'Low', 'Medium', 'High'\\n\",\n",
    "    \"# ======================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"def calculate_suitability_score(row, risk_profile):\\n\",\n",
    "    \"    # --- Robust Normalization ---\\n\",\n",
    "    \"    exp_return_norm = max(0, min(1, row['Expected Annual Return (%)'] / 50))\\n\",\n",
    "    \"    sharpe_norm = max(0, min(1, row['Sharpe Ratio'] / 3))\\n\",\n",
    "    \"    volatility_norm = max(0, min(1, 1 - (row['Annualized Volatility (%)'] / 50)))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # --- Scoring Logic based on Your Risk Tolerance ---\\n\",\n",
    "    \"    if risk_profile == 'Low':\\n\",\n",
    "    \"        score = (sharpe_norm * 0.6) + (volatility_norm * 0.3) + (exp_return_norm * 0.1)\\n\",\n",
    "    \"    elif risk_profile == 'Medium':\\n\",\n",
    "    \"        score = (exp_return_norm * 0.4) + (sharpe_norm * 0.4) + (volatility_norm * 0.2)\\n\",\n",
    "    \"    elif risk_profile == 'High':\\n\",\n",
    "    \"        score = (exp_return_norm * 0.7) + (sharpe_norm * 0.2) + (volatility_norm * 0.1)\\n\",\n",
    "    \"    else: # Default to medium\\n\",\n",
    "    \"        score = (exp_return_norm * 0.4) + (sharpe_norm * 0.4) + (volatility_norm * 0.2)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    if row['Fund Age (Yrs)'] > INVESTMENT_HORIZON_YEARS:\\n\",\n",
    "    \"        score *= 1.1\\n\",\n",
    "    \"    return score\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Execution ---\\n\",\n",
    "    \"recommendation_df = master_analysis_df.copy()\\n\",\n",
    "    \"recommendation_df = recommendation_df[recommendation_df['Fund Age (Yrs)'] >= 1].dropna(subset=['Sharpe Ratio'])\\n\",\n",
    "    \"recommendation_df['Suitability Score'] = recommendation_df.apply(\\n\",\n",
    "    \"    calculate_suitability_score, axis=1, risk_profile=RISK_TOLERANCE\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Split recommendations into Core and Specialized/Thematic ---\\n\",\n",
    "    \"# Define keywords for specialized funds\\n\",\n",
    "    \"specialized_keywords = [\\n\",\n",
    "    \"    'DEFENCE', 'PHARMA', 'HEALTHCARE', 'TECHNOLOGY', 'INFRASTRUCTURE',\\n\",\n",
    "    \"    'BANKING', 'FINANCIAL SERVICES', 'PSU', 'COMMODITIES', 'CONSUMPTION',\\n\",\n",
    "    \"    'ENERGY', 'AUTO', 'CHILDREN', 'BENEFIT', 'RETIREMENT', 'SAVER'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"specialized_pattern = '|'.join(specialized_keywords)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a boolean mask to identify specialized funds\\n\",\n",
    "    \"is_specialized_mask = recommendation_df['scheme_name'].str.contains(specialized_pattern, case=False, na=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create two separate dataframes\\n\",\n",
    "    \"core_recommendations = recommendation_df[~is_specialized_mask]\\n\",\n",
    "    \"specialized_recommendations = recommendation_df[is_specialized_mask]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort both dataframes by Suitability Score\\n\",\n",
    "    \"core_recommendations = core_recommendations.sort_values('Suitability Score', ascending=False)\\n\",\n",
    "    \"specialized_recommendations = specialized_recommendations.sort_values('Suitability Score', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Display the Final Recommendations ---\\n\",\n",
    "    \"recommendation_cols = [\\n\",\n",
    "    \"    'scheme_name', 'Suitability Score', 'Expected Annual Return (%)', \\n\",\n",
    "    \"    'Annualized Volatility (%)', 'Sharpe Ratio', 'Fund Age (Yrs)'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'\\\\n--- Recommendations for a {INVESTMENT_HORIZON_YEARS}-Year Horizon with {RISK_TOLERANCE} Risk Tolerance ---\\\\n')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display Top Core Funds\\n\",\n",
    "    \"print(\\\"\\\\n--- Top 15 Core Diversified Funds ---\\\")\\n\",\n",
    "    \"print(\\\"These are generally suitable as the main part of a portfolio.\\\")\\n\",\n",
    "    \"display(core_recommendations[recommendation_cols].head(15))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display Top Specialized Funds\\n\",\n",
    "    \"if not specialized_recommendations.empty:\\n\",\n",
    "    \"    print(\\\"\\\\n\\\\n--- Top 5 Specialized & Thematic Funds ---\\\")\\n\",\n",
    "    \"    print(\\\"These funds focus on specific sectors. They have performed exceptionally well but carry higher concentration risk.\\\")\\n\",\n",
    "    \"    display(specialized_recommendations[recommendation_cols].head(5))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Part 3: Diversification Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"A well-diversified portfolio contains funds that don't all move together. Use the **Dendrogram** to pick funds from different color clusters. Use the **Correlation Heatmap** to see the exact relationship strength (lower is better for diversification).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ“ˆ Part 3a: Diversification Analysis via Clustering and Correlation\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Enable portfolio construction that avoids over-concentration by analyzing **co-movement between top fund returns**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Tools Used:**\\n\",\n",
    "    \"- **Hierarchical Clustering (Dendrogram):** Visual groupings of similar funds\\n\",\n",
    "    \"- **Correlation Heatmap:** Precise relationship quantification\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"Helps investors select funds that do not all rise and fall together, increasing portfolio stability through **uncorrelated assets**.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ”¬ Part 3b: Technical Deep Dive into the Top Fund\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Zoom in on the **top recommended core fund** to assess its NAV behavior, momentum, volatility trends, and potential entry signals.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Techniques Used:**\\n\",\n",
    "    \"- SMAs (20, 50, 200-day)\\n\",\n",
    "    \"- Bollinger Bands\\n\",\n",
    "    \"- RSI (Relative Strength Index)\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"These indicators help detect overbought/oversold signals and general trend directionâ€”useful for tactical timing decisions.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Part 4: Diversification Analysis for Your Recommended Funds ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"# This code assumes the 'core_recommendations' and 'specialized_recommendations' DataFrames exist from the previous cell.\\n\",\n",
    "    \"\\n\",\n",
    "    \"# We will create a combined list of top funds for a holistic diversification view\\n\",\n",
    "    \"top_core = core_recommendations.head(10)['scheme_name']\\n\",\n",
    "    \"top_specialized = specialized_recommendations.head(5)['scheme_name']\\n\",\n",
    "    \"top_funds_to_analyze = pd.concat([top_core, top_specialized]).unique()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n--- Analyzing Diversification for Top Recommended Funds ---\\\")\\n\",\n",
    "    \"print(f\\\"This analysis helps you pick funds that don't all move in the same direction.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Pivot the data to get daily returns for each fund in columns\\n\",\n",
    "    \"# This correctly handles funds with different histories.\\n\",\n",
    "    \"returns_pivot = df[df['scheme_name'].isin(top_funds_to_analyze)].pivot_table(\\n\",\n",
    "    \"    index='date', columns='scheme_name', values='daily_return'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Data Check ---\\n\",\n",
    "    \"min_observations = 60  # Require at least ~3 months of common data\\n\",\n",
    "    \"min_funds = 3          # Minimum number of funds to analyze\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Keep only the days where at least `min_funds` have data\\n\",\n",
    "    \"valid_days = returns_pivot.dropna(thresh=min_funds)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(valid_days) < min_observations:\\n\",\n",
    "    \"    print(f\\\"\\\\nWarning: Not enough overlapping historical data ({len(valid_days)} days) found for the top recommended funds.\\\")\\n\",\n",
    "    \"    print(f\\\"A reliable diversification analysis could not be performed.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    # Calculate correlation on the data from the valid, overlapping days\\n\",\n",
    "    \"    correlation_matrix = valid_days.corr()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # --- 1. Dendrogram (The 'Family Tree' of Your Funds) ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Fund Cluster Analysis (Dendrogram) ---\\\")\\n\",\n",
    "    \"    print(\\\"Tip: For good diversification, try to pick funds from different main branches (colors).\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(20, 8))\\n\",\n",
    "    \"    plt.title(f'Hierarchical Clustering of Recommended Funds', fontsize=18, pad=20)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # The linkage function creates the clusters\\n\",\n",
    "    \"    linked = hierarchy.linkage(1 - correlation_matrix, method='ward')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    hierarchy.dendrogram(linked, \\n\",\n",
    "    \"                       labels=correlation_matrix.columns, \\n\",\n",
    "    \"                       leaf_rotation=90, \\n\",\n",
    "    \"                       leaf_font_size=10)\\n\",\n",
    "    \"    plt.ylabel('Cluster Distance (Higher means more different)', fontsize=12)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- 2. Correlation Heatmap (The Detailed View) ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Correlation Matrix (Heatmap) ---\\\")\\n\",\n",
    "    \"    print(\\\"This shows the exact relationship strength. Lower numbers (blue/cooler colors) are better for diversification.\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(16, 14))\\n\",\n",
    "    \"    sns.heatmap(correlation_matrix, \\n\",\n",
    "    \"              annot=True, \\n\",\n",
    "    \"              cmap='coolwarm', \\n\",\n",
    "    \"              center=0,\\n\",\n",
    "    \"              vmin=-1, \\n\",\n",
    "    \"              vmax=1,\\n\",\n",
    "    \"              fmt='.2f', \\n\",\n",
    "    \"              linewidths=.5)\\n\",\n",
    "    \"    plt.title(f'Correlation Matrix of Daily Returns', fontsize=18, pad=20)\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # --- 3. Actionable Summary ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Diversification Summary ---\\\")\\n\",\n",
    "    \"    # Calculate average correlation from the upper triangle of the matrix to avoid duplicates\\n\",\n",
    "    \"    avg_corr = correlation_matrix.values[np.triu_indices_from(correlation_matrix, k=1)].mean()\\n\",\n",
    "    \"    print(f\\\"The average correlation among your selected funds is: {avg_corr:.2f}\\\")\\n\",\n",
    "    \"    if avg_corr > 0.7:\\n\",\n",
    "    \"        print(\\\"Suggestion: Your selected funds are quite similar. Consider replacing one with a fund from a different cluster.\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"Suggestion: This looks like a well-diversified selection.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Part 5: Individual Fund Deep-Dive ---\\n\",\n",
    "    \"print(\\\"\\\\n\\\\n--- Individual Fund Deep-Dive ---\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select the top recommended CORE fund for a detailed look\\n\",\n",
    "    \"if not core_recommendations.empty:\\n\",\n",
    "    \"    selected_fund_name = core_recommendations.iloc[0]['scheme_name']\\n\",\n",
    "    \"    print(f\\\"Analyzing the top recommended core fund: {selected_fund_name}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    fund_df = df[df['scheme_name'] == selected_fund_name].set_index('date').copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Calculate Technical Indicators\\n\",\n",
    "    \"    fund_df['50_day_sma'] = fund_df['nav'].rolling(window=50).mean()\\n\",\n",
    "    \"    fund_df['200_day_sma'] = fund_df['nav'].rolling(window=200).mean()\\n\",\n",
    "    \"    fund_df['20_day_sma'] = fund_df['nav'].rolling(window=20).mean()\\n\",\n",
    "    \"    fund_df['20_day_std'] = fund_df['nav'].rolling(window=20).std()\\n\",\n",
    "    \"    fund_df['bollinger_upper'] = fund_df['20_day_sma'] + (fund_df['20_day_std'] * 2)\\n\",\n",
    "    \"    fund_df['bollinger_lower'] = fund_df['20_day_sma'] - (fund_df['20_day_std'] * 2)\\n\",\n",
    "    \"    delta = fund_df['nav'].diff()\\n\",\n",
    "    \"    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\\n\",\n",
    "    \"    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\\n\",\n",
    "    \"    rs = gain / loss\\n\",\n",
    "    \"    fund_df['rsi'] = 100 - (100 / (1 + rs))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Create Subplots for visualization\\n\",\n",
    "    \"    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\\n\",\n",
    "    \"    fig.suptitle(f'Technical Analysis for {selected_fund_name}', fontsize=20)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Plot 1: NAV, SMAs, and Bollinger Bands\\n\",\n",
    "    \"    ax1.plot(fund_df.index, fund_df['nav'], label='NAV', color='blue', alpha=0.8)\\n\",\n",
    "    \"    ax1.plot(fund_df.index, fund_df['50_day_sma'], label='50-Day SMA', color='orange', linestyle='--')\\n\",\n",
    "    \"    ax1.plot(fund_df.index, fund_df['200_day_sma'], label='200-Day SMA', color='red', linestyle='--')\\n\",\n",
    "    \"    ax1.fill_between(fund_df.index, fund_df['bollinger_upper'], fund_df['bollinger_lower'], color='gray', alpha=0.2, label='Bollinger Bands')\\n\",\n",
    "    \"    ax1.set_ylabel('NAV (â‚¹)')\\n\",\n",
    "    \"    ax1.set_title('Price Trend & Volatility')\\n\",\n",
    "    \"    ax1.legend()\\n\",\n",
    "    \"    ax1.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Plot 2: RSI\\n\",\n",
    "    \"    ax2.plot(fund_df.index, fund_df['rsi'], label='RSI', color='purple')\\n\",\n",
    "    \"    ax2.axhline(70, linestyle='--', color='red', alpha=0.5, label='Overbought (70)')\\n\",\n",
    "    \"    ax2.axhline(30, linestyle='--', color='green', alpha=0.5, label='Oversold (30)')\\n\",\n",
    "    \"    ax2.set_ylabel('RSI')\\n\",\n",
    "    \"    ax2.set_title('Relative Strength Index (Momentum)')\\n\",\n",
    "    \"    ax2.set_ylim(0, 100)\\n\",\n",
    "    \"    ax2.legend()\\n\",\n",
    "    \"    ax2.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    plt.xlabel('Date')\\n\",\n",
    "    \"    plt.tight_layout(rect=[0, 0, 1, 0.97])\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nDeep-dive analysis skipped as no core funds were recommended.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ’¼ Part 4: Personalized Final Portfolio Recommendation\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective:** \\n\",\n",
    "    \"Construct a **final diversified investment portfolio** of high-suitability mutual funds that also **minimize internal correlation**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Steps:**\\n\",\n",
    "    \"- Start with highest scoring fund\\n\",\n",
    "    \"- Iteratively add low-correlation funds\\n\",\n",
    "    \"- Optionally include a thematic/specialized satellite fund\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Why:** \\n\",\n",
    "    \"This creates a **smart portfolio**â€”well-performing but not over-exposed to a single trend or asset behavior. We also ensure that recommendations are practical (i.e., limited to funds with enough historical data).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Part 6: Your Personalized Diversified Portfolio Recommendation (Corrected) ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*80)\\n\",\n",
    "    \"print(\\\"== YOUR FINAL DIVERSIFIED PORTFOLIO RECOMMENDATION ==\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*80)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# This code assumes 'core_recommendations', 'specialized_recommendations', \\n\",\n",
    "    \"# and 'correlation_matrix' exist from the previous cells.\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- DEFINITIVE FIX for KeyError ---\\n\",\n",
    "    \"# First, ensure we only consider funds that are actually in our correlation matrix.\\n\",\n",
    "    \"# This prevents errors if a recommended fund was too new for the diversification analysis.\\n\",\n",
    "    \"portfolio_candidates = core_recommendations[core_recommendations['scheme_name'].isin(correlation_matrix.columns)]\\n\",\n",
    "    \"specialized_candidates = specialized_recommendations[specialized_recommendations['scheme_name'].isin(correlation_matrix.columns)]\\n\",\n",
    "    \"# --- End of Fix ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set the desired size of your core portfolio\\n\",\n",
    "    \"CORE_PORTFOLIO_SIZE = 3\\n\",\n",
    "    \"DIVERSIFICATION_THRESHOLD = 0.85 # We want funds with correlation less than this\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Start with your best fund as the anchor\\n\",\n",
    "    \"diversified_portfolio = []\\n\",\n",
    "    \"if not portfolio_candidates.empty:\\n\",\n",
    "    \"    diversified_portfolio.append(portfolio_candidates.iloc[0])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Iteratively add the most different funds from our filtered list of candidates\\n\",\n",
    "    \"for index, candidate_fund in portfolio_candidates.iloc[1:].iterrows():\\n\",\n",
    "    \"    if len(diversified_portfolio) >= CORE_PORTFOLIO_SIZE:\\n\",\n",
    "    \"        break\\n\",\n",
    "    \"\\n\",\n",
    "    \"    is_different_enough = True\\n\",\n",
    "    \"    # Check the candidate against every fund already in our portfolio\\n\",\n",
    "    \"    for selected_fund in diversified_portfolio:\\n\",\n",
    "    \"        correlation_value = correlation_matrix.loc[selected_fund['scheme_name'], candidate_fund['scheme_name']]\\n\",\n",
    "    \"        if correlation_value > DIVERSIFICATION_THRESHOLD:\\n\",\n",
    "    \"            is_different_enough = False\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if is_different_enough:\\n\",\n",
    "    \"        diversified_portfolio.append(candidate_fund)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert the list of funds into a clean DataFrame\\n\",\n",
    "    \"portfolio_df = pd.DataFrame(diversified_portfolio)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- The Report (in Simple Terms) ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nHello! Based on your goal of a \\\"\\n\",\n",
    "    \"      f\\\"{INVESTMENT_HORIZON_YEARS}-year investment with {RISK_TOLERANCE} risk, here is a sample portfolio designed for you.\\\")\\n\",\n",
    "    \"print(\\\"\\\\nThe main goal of this portfolio is **diversification**. This simply means not putting all your eggs in one basket. We've selected funds that have performed well but don't always move in the same direction, which helps to lower your overall risk.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not portfolio_df.empty:\\n\",\n",
    "    \"    print(\\\"\\\\n--- Your Recommended Core Portfolio ---\\\\n\\\")\\n\",\n",
    "    \"    display(portfolio_df[['scheme_name', 'Suitability Score', 'Expected Annual Return (%)', 'Annualized Volatility (%)']])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\\"\\\\n--- Why These Funds Were Chosen ---\\\\n\\\")\\n\",\n",
    "    \"    for index, fund in portfolio_df.iterrows():\\n\",\n",
    "    \"        if index == 0:\\n\",\n",
    "    \"            print(f\\\"1. **{fund['scheme_name']}**: This is your 'Anchor' fund. It's the top-ranked fund based on your goals and forms the foundation of your investment.\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Get the average correlation of this fund to the ones selected before it\\n\",\n",
    "    \"            avg_corr = correlation_matrix.loc[fund['scheme_name'], portfolio_df.iloc[0:index]['scheme_name']].mean()\\n\",\n",
    "    \"            print(f\\\"\\\\n{index + 1}. **{fund['scheme_name']}**: This fund was added because it's a strong performer that is also quite different from the others already in the portfolio (average correlation of only {avg_corr:.2f}). This adds a good layer of diversification.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Optional: Add a 'Satellite' fund for extra growth ---\\n\",\n",
    "    \"    if not specialized_candidates.empty:\\n\",\n",
    "    \"        top_specialized_fund = specialized_candidates.iloc[0]\\n\",\n",
    "    \"        # Check if the specialized fund is different enough from the core portfolio\\n\",\n",
    "    \"        avg_corr_specialized = correlation_matrix.loc[top_specialized_fund['scheme_name'], portfolio_df['scheme_name']].mean()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if avg_corr_specialized < DIVERSIFICATION_THRESHOLD:\\n\",\n",
    "    \"            print(\\\"\\\\n\\\\n--- Optional 'Satellite' Fund for Higher Growth ---\\\\n\\\")\\n\",\n",
    "    \"            print(\\\"For investors comfortable with more risk, you can add a small portion of your investment to a specialized fund. These are less diversified but have very high growth potential.\\\")\\n\",\n",
    "    \"            display(pd.DataFrame([top_specialized_fund])[['scheme_name', 'Suitability Score', 'Expected Annual Return (%)']])\\n\",\n",
    "    \"            print(f\\\"This fund focuses on a specific theme and has a low correlation ({avg_corr_specialized:.2f}) to your core portfolio, making it a good high-risk, high-reward addition.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nCould not generate a diversified portfolio. This may be due to a lack of funds with sufficient overlapping history.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\\n**Disclaimer:** This is a statistically generated recommendation based on historical data. Past performance is not indicative of future results. Please consider consulting with a financial advisor.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## âœ… Final Thoughts & Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook presents a fully functional **Mutual Fund Analysis & Recommendation Engine**. Here's what we achieved:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- ðŸ§® **Back-End Analysis Engine:** Calculated detailed historical and statistical performance profiles using efficient SQL and pandas logic.\\n\",\n",
    "    \"- ðŸ§  **Custom Recommendation Engine:** Personalized fund recommendations based on user-defined risk and timeline.\\n\",\n",
    "    \"- ðŸ“Š **Visual Diversification Tools:** Used clustering and correlation to enable smarter fund selection.\\n\",\n",
    "    \"- ðŸ”¬ **Technical Deep-Dive:** Fund-specific indicators helped investors interpret timing and performance signals.\\n\",\n",
    "    \"- ðŸ’¼ **Portfolio Builder:** Automated selection of low-correlation, high-return mutual funds for a diversified investment basket.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**This project demonstrates how data science can drive real-world, investor-facing financial intelligence.**\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fbb2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dont_know.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%writefile dont_know.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Part 1: Setup and Memory-Safe Data Loading\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Goal:** Load all necessary libraries and use a smart SQL query to load only data for open-ended, accessible funds, preventing memory errors and ensuring data relevance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from sqlalchemy import create_engine\\n\",\n",
    "    \"from config import db_config # Assuming config.py is in the same directory\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from scipy.cluster import hierarchy\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- Setup plotting and display styles ---\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (18, 9)\\n\",\n",
    "    \"pd.set_option('display.float_format', lambda x: f'{x:.2f}')\\n\",\n",
    "    \"pd.set_option('display.width', 1000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- MEMORY-SAFE DATA LOADING FROM SQL ---\\n\",\n",
    "    \"print(\\\"--- Step 1: Connecting to SQL Server to identify unsuitable funds to exclude ---\\\")\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    engine = create_engine(db_config.url)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Query 1: Get the small, unique list of all fund schemes\\n\",\n",
    "    \"    all_funds_query = \\\"SELECT DISTINCT scheme_code, scheme_name FROM nav_data\\\"\\n\",\n",
    "    \"    all_funds_df = pd.read_sql(all_funds_query, engine)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Comprehensive keyword list to filter funds not for regular investors\\n\",\n",
    "    \"    filter_keywords = [\\n\",\n",
    "    \"        'FMP', 'FIXED MATURITY', 'FIXED TERM', 'SERIES',\\n\",\n",
    "    \"        'INTERVAL FUND', 'CAPITAL PROTECTION', 'CLOSED ENDED',\\n\",\n",
    "    \"        'CLOSE ENDED', 'CLOSE-ENDED', 'CAP PROTECTION', 'LIMITED OFFER',\\n\",\n",
    "    \"        'NFO', 'OPPORTUNITY FUND', 'MATURITY', 'TARGET MATURITY',\\n\",\n",
    "    \"        'SEGREGATED PORTFOLIO', 'LOCK-IN', 'LIMITED PERIOD'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    keyword_pattern = '|'.join(filter_keywords)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    all_funds_df['scheme_name'] = all_funds_df['scheme_name'].astype(str)\\n\",\n",
    "    \"    unsuitable_funds = all_funds_df[all_funds_df['scheme_name'].str.contains(keyword_pattern, case=False, na=False)]\\n\",\n",
    "    \"    schemes_to_exclude = unsuitable_funds['scheme_code'].tolist()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Identified {len(schemes_to_exclude)} unsuitable schemes to exclude.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Step 2: Build a specific SQL query to load ONLY the required historical data ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 2: Loading historical data for suitable, open-ended funds from SQL ---\\\")\\n\",\n",
    "    \"    if schemes_to_exclude:\\n\",\n",
    "    \"        exclude_list_str = \\\", \\\".join([f\\\"'{code}'\\\" for code in schemes_to_exclude])\\n\",\n",
    "    \"        data_query = f\\\"SELECT * FROM nav_data WHERE scheme_code NOT IN ({exclude_list_str})\\\"\\n\",\n",
    "    \"        df = pd.read_sql(data_query, engine, parse_dates=['date'])\\n\",\n",
    "    \"        print(f\\\"Successfully loaded a manageable subset of {len(df)} records.\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"No funds to exclude. Loading entire dataset.\\\")\\n\",\n",
    "    \"        df = pd.read_sql(\\\"SELECT * FROM nav_data\\\", engine, parse_dates=['date'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"An error occurred during data loading: {e}\\\")\\n\",\n",
    "    \"    df = pd.DataFrame()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Part 2: The Main Analysis Engine\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Goal:** Take the filtered data from Part 1 and perform the complete, robust analysis. This version contains the definitive fix for the `ValueError: left keys must be sorted`.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty:\\n\",\n",
    "    \"    print(\\\"\\\\n--- Step 3: Calculating comprehensive metrics for all funds ---\\\")\\n\",\n",
    "    \"    df['daily_return'] = df.groupby('scheme_code')['nav'].pct_change()\\n\",\n",
    "    \"    df_sorted = df.sort_values(['scheme_code', 'date'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Prepare clean, sorted base dataframes\\n\",\n",
    "    \"    first_navs = df_sorted.loc[df_sorted.groupby('scheme_code')['date'].idxmin()]\\n\",\n",
    "    \"    # Sort 'latest_navs' by the 'by' key ('scheme_code') to ensure stability for the loop\\n\",\n",
    "    \"    latest_navs = df_sorted.loc[df_sorted.groupby('scheme_code')['date'].idxmax()].sort_values('scheme_code').reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    master_analysis_df = latest_navs[['scheme_code', 'scheme_name', 'date', 'nav']].rename(columns={'date': 'latest_date', 'nav': 'latest_nav'})\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df.merge(first_navs[['scheme_code', 'date', 'nav']], on='scheme_code', how='inner')\\n\",\n",
    "    \"    master_analysis_df.rename(columns={'date': 'inception_date', 'nav': 'inception_nav'}, inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Activity Filter (remove funds that haven't reported recently)\\n\",\n",
    "    \"    last_valid_date = pd.Timestamp.now() - pd.DateOffset(days=30)\\n\",\n",
    "    \"    activity_mask = master_analysis_df['latest_date'] >= last_valid_date\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df[activity_mask].copy()\\n\",\n",
    "    \"    print(f\\\"Final number of active, open-ended funds for analysis: {len(master_analysis_df)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Historical Performance (CAGR) with guaranteed sorting ---\\n\",\n",
    "    \"    def calculate_cagr(start_value, end_value, years):\\n\",\n",
    "    \"        years = pd.to_numeric(years, errors='coerce')\\n\",\n",
    "    \"        start_value = pd.to_numeric(start_value, errors='coerce')\\n\",\n",
    "    \"        end_value = pd.to_numeric(end_value, errors='coerce')\\n\",\n",
    "    \"        result = pd.Series(np.nan, index=start_value.index)\\n\",\n",
    "    \"        valid_mask = (start_value > 0) & (end_value > 0) & (years > 0.25)\\n\",\n",
    "    \"        s, e, y = start_value[valid_mask], end_value[valid_mask], years[valid_mask]\\n\",\n",
    "    \"        result.loc[valid_mask] = ((e / s) ** (1 / y) - 1) * 100\\n\",\n",
    "    \"        return result\\n\",\n",
    "    \"\\n\",\n",
    "    \"    right_df_for_merge = df_sorted[['date', 'scheme_code', 'nav']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for years in [1, 3, 5, 10]:\\n\",\n",
    "    \"        target_date_col = f'date_{years}y_ago'\\n\",\n",
    "    \"        nav_col = f'nav_{years}y_ago'\\n\",\n",
    "    \"        master_analysis_df[target_date_col] = master_analysis_df['latest_date'] - pd.DateOffset(years=years)\\n\",\n",
    "    \"        left_df = master_analysis_df[['scheme_code', target_date_col]].sort_values(by=['scheme_code', target_date_col])\\n\",\n",
    "    \"        period_navs = pd.merge_asof(left_df, right_df_for_merge, left_on=target_date_col, right_on='date', by='scheme_code', direction='nearest')\\n\",\n",
    "    \"        master_analysis_df = master_analysis_df.merge(period_navs[['scheme_code', 'nav']].rename(columns={'nav': nav_col}), on='scheme_code', how='left')\\n\",\n",
    "    \"        master_analysis_df[f'{years}Y Return (%)'] = calculate_cagr(master_analysis_df[nav_col], master_analysis_df['latest_nav'], years)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    master_analysis_df['Fund Age (Yrs)'] = (master_analysis_df['latest_date'] - master_analysis_df['inception_date']).dt.days / 365.25\\n\",\n",
    "    \"    master_analysis_df['Since Inception Return (%)'] = calculate_cagr(master_analysis_df['inception_nav'], master_analysis_df['latest_nav'], master_analysis_df['Fund Age (Yrs)'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Statistical Profile (Risk and Expectation) ---\\n\",\n",
    "    \"    daily_returns_grouped = df.groupby('scheme_code')['daily_return']\\n\",\n",
    "    \"    mean_returns = daily_returns_grouped.mean().to_frame(name='mean')\\n\",\n",
    "    \"    std_returns = daily_returns_grouped.std().to_frame(name='std')\\n\",\n",
    "    \"    stats_df = mean_returns.join(std_returns).reset_index()\\n\",\n",
    "    \"    stats_df['Expected Annual Return (%)'] = stats_df['mean'] * 252 * 100\\n\",\n",
    "    \"    stats_df['Annualized Volatility (%)'] = stats_df['std'] * np.sqrt(252) * 100\\n\",\n",
    "    \"    risk_free_rate_daily = (1.04 ** (1/252)) - 1\\n\",\n",
    "    \"    stats_df['Sharpe Ratio'] = (stats_df['mean'] - risk_free_rate_daily) / stats_df['std'] * np.sqrt(252)\\n\",\n",
    "    \"    master_analysis_df = master_analysis_df.merge(stats_df[['scheme_code', 'Expected Annual Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio']], on='scheme_code', how='left')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Final Display Table ---\\n\",\n",
    "    \"    display_cols = [\\n\",\n",
    "    \"        'scheme_name', 'Fund Age (Yrs)', 'Since Inception Return (%)', '10Y Return (%)', '5Y Return (%)',\\n\",\n",
    "    \"        '3Y Return (%)', '1Y Return (%)', 'Expected Annual Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    display_df = master_analysis_df[display_cols].sort_values('5Y Return (%)', ascending=False).copy()\\n\",\n",
    "    \"    for col in ['10Y Return (%)', '5Y Return (%)', '3Y Return (%)', '1Y Return (%)', 'Since Inception Return (%)']:\\n\",\n",
    "    \"        display_df[col] = display_df[col].apply(lambda x: f\\\"{x:.2f}\\\" if pd.notna(x) else '-')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\\"\\\\n--- Master Analysis Table: Curated for Investors ---\\\")\\n\",\n",
    "    \"    display(display_df.head(20))\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nAnalysis skipped because no data was loaded.\\\")\\n\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1bdcfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting forecasting_final.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%writefile forecasting_final.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Predictive Forecasting with Growth Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Data Loading\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from sqlalchemy import create_engine\\n\",\n",
    "    \"from config import db_config\\n\",\n",
    "    \"from prophet import Prophet\\n\",\n",
    "    \"from statsmodels.tsa.holtwinters import ExponentialSmoothing\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (15, 7)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries loaded.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"engine = create_engine(db_config.url)\\n\",\n",
    "    \"query = \\\"SELECT * FROM nav_data\\\"\\n\",\n",
    "    \"df = pd.read_sql(query, engine)\\n\",\n",
    "    \"df['date'] = pd.to_datetime(df['date'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(df)} records and {df['scheme_code'].nunique()} unique funds from the database.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Select a Fund to Forecast\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fund_list = sorted(df['scheme_name'].unique())\\n\",\n",
    "    \"selected_fund_name = 'HDFC Flexi Cap Fund - Direct Plan - Growth Option'\\n\",\n",
    "    \"if selected_fund_name not in fund_list:\\n\",\n",
    "    \"    selected_fund_name = fund_list[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nAnalyzing fund: {selected_fund_name}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fund_df_raw = df[df['scheme_name'] == selected_fund_name][['date', 'nav']].copy()\\n\",\n",
    "    \"fund_df_raw.drop_duplicates(subset=['date'], keep='last', inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"MIN_DATA_POINTS = 365 # We need at least a year of data for yearly analysis\\n\",\n",
    "    \"has_enough_data = len(fund_df_raw) >= MIN_DATA_POINTS\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not has_enough_data:\\n\",\n",
    "    \"    print(f\\\"ðŸ›‘ ANALYSIS HALTED: Not enough data for the selected fund. Need at least {MIN_DATA_POINTS} days of data for yearly analysis.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"âœ… Sufficient data found. Proceeding with forecast...\\\")\\n\",\n",
    "    \"    fund_df = fund_df_raw.sort_values('date').set_index('date').asfreq('D').fillna(method='ffill').reset_index()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Forecasting Models & Detailed Visualization\\n\",\n",
    "    \"This section will only run if the selected fund has enough data. It now includes the main forecast plot, a bar chart for historical growth, and a table with specific future NAV predictions.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if has_enough_data:\\n\",\n",
    "    \"    # --- Run Forecasting Models (Prophet & Holt-Winters) ---\\n\",\n",
    "    \"    print(\\\"\\\\nFitting forecasting models...\\\")\\n\",\n",
    "    \"    prophet_df = fund_df.rename(columns={'date': 'ds', 'nav': 'y'})\\n\",\n",
    "    \"    model_prophet = Prophet(daily_seasonality=True).fit(prophet_df)\\n\",\n",
    "    \"    future_prophet = model_prophet.make_future_dataframe(periods=365)\\n\",\n",
    "    \"    forecast_prophet = model_prophet.predict(future_prophet)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    hw_df = fund_df.set_index('date')\\n\",\n",
    "    \"    model_hw = ExponentialSmoothing(hw_df['nav'], trend='add', seasonal='add', seasonal_periods=365).fit()\\n\",\n",
    "    \"    forecast_hw = model_hw.forecast(365)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # --- Create Ensemble Forecast ---\\n\",\n",
    "    \"    print(\\\"Creating ensemble forecast...\\\")\\n\",\n",
    "    \"    prophet_future_values = forecast_prophet[-365:]['yhat']\\n\",\n",
    "    \"    hw_future_values = forecast_hw\\n\",\n",
    "    \"    ensemble_forecast = (prophet_future_values.values + hw_future_values.values) / 2\\n\",\n",
    "    \"    forecast_dates = pd.date_range(start=fund_df['date'].iloc[-1], periods=365 + 1)[1:]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- NEW: Create a Summary Table of Forecasted Values ---\\n\",\n",
    "    \"    print(\\\"\\\\n--- Key Forecasted NAV Values ---\\\")\\n\",\n",
    "    \"    forecast_summary = pd.DataFrame({\\n\",\n",
    "    \"        'Timeframe': ['1 Month', '3 Months', '6 Months', '1 Year'],\\n\",\n",
    "    \"        'Date': [\\n\",\n",
    "    \"            forecast_dates[29].date(),\\n\",\n",
    "    \"            forecast_dates[89].date(),\\n\",\n",
    "    \"            forecast_dates[179].date(),\\n\",\n",
    "    \"            forecast_dates[364].date()\\n\",\n",
    "    \"        ],\\n\",\n",
    "    \"        'Forecasted NAV (â‚¹)': [\\n\",\n",
    "    \"            ensemble_forecast[29],\\n\",\n",
    "    \"            ensemble_forecast[89],\\n\",\n",
    "    \"            ensemble_forecast[179],\\n\",\n",
    "    \"            ensemble_forecast[364]\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    forecast_summary['Forecasted NAV (â‚¹)'] = forecast_summary['Forecasted NAV (â‚¹)'].round(2)\\n\",\n",
    "    \"    display(forecast_summary.set_index('Timeframe'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Create Figure with Two Subplots ---\\n\",\n",
    "    \"    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 14), gridspec_kw={'height_ratios': [3, 2]})\\n\",\n",
    "    \"    fig.suptitle(f'In-Depth Analysis for: {selected_fund_name}', fontsize=20)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Plot 1: The Main Forecast ---\\n\",\n",
    "    \"    ax1.plot(fund_df['date'], fund_df['nav'], label='Historical NAV')\\n\",\n",
    "    \"    ax1.plot(forecast_dates, ensemble_forecast, color='purple', linestyle='--', label='Ensemble Forecast (1-Year)')\\n\",\n",
    "    \"    ax1.set_title('Historical NAV and Future Forecast')\\n\",\n",
    "    \"    ax1.set_ylabel('NAV (â‚¹)')\\n\",\n",
    "    \"    ax1.legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # --- Plot 2: Year-over-Year Growth Bar Chart ---\\n\",\n",
    "    \"    print(\\\"\\\\nCalculating year-over-year growth...\\\")\\n\",\n",
    "    \"    yearly_df = fund_df.set_index('date').resample('A-DEC').last()\\n\",\n",
    "    \"    yearly_df['yearly_growth_pct'] = yearly_df['nav'].pct_change() * 100\\n\",\n",
    "    \"    yearly_df.dropna(inplace=True)\\n\",\n",
    "    \"    yearly_df.index = yearly_df.index.year\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    colors = ['green' if x > 0 else 'red' for x in yearly_df['yearly_growth_pct']]\\n\",\n",
    "    \"    sns.barplot(x=yearly_df.index, y=yearly_df['yearly_growth_pct'], ax=ax2, palette=colors)\\n\",\n",
    "    \"    ax2.set_title('Historical Year-over-Year Growth')\\n\",\n",
    "    \"    ax2.set_xlabel('Year')\\n\",\n",
    "    \"    ax2.set_ylabel('Growth (%)')\\n\",\n",
    "    \"    for p in ax2.patches:\\n\",\n",
    "    \"        ax2.annotate(f'{p.get_height():.1f}%', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 9), textcoords='offset points')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d1a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_inspection.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_inspection.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Data Inspection and Diagnostics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"This notebook traces the data for a single fund through the entire pipeline to find any discrepancies.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from sqlalchemy import create_engine\\n\",\n",
    "    \"from config import db_config\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"\\n\",\n",
    "    \"# The fund name we are investigating\\n\",\n",
    "    \"TARGET_FUND_NAME = 'Axis Bluechip Fund - Direct Plan - Growth'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Step 1: Inspect the Raw Source Data\\n\",\n",
    "    \"**Goal:** Let's get the master list of all funds directly from the API to find the **exact** `schemeName` and `schemeCode` for our target fund. A tiny difference in the name can cause it to be missed.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"Fetching master fund list from API...\\\")\\n\",\n",
    "    \"all_funds_url = \\\"https://api.mfapi.in/mf\\\"\\n\",\n",
    "    \"response = requests.get(all_funds_url)\\n\",\n",
    "    \"all_funds_df = pd.DataFrame(response.json())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Search for our target fund in the raw list\\n\",\n",
    "    \"target_fund_info = all_funds_df[all_funds_df['schemeName'].str.contains('Axis Bluechip Fund - Direct Plan - Growth', case=False)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"--- Raw Fund Information from API Source ---\\\")\\n\",\n",
    "    \"if not target_fund_info.empty:\\n\",\n",
    "    \"    print(\\\"Fund found at the source!\\\")\\n\",\n",
    "    \"    display(target_fund_info)\\n\",\n",
    "    \"    # Store the exact code and name for the next steps\\n\",\n",
    "    \"    exact_scheme_code = target_fund_info.iloc[0]['schemeCode']\\n\",\n",
    "    \"    exact_scheme_name = target_fund_info.iloc[0]['schemeName']\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"ðŸ›‘ CRITICAL: Fund not found in the master list from the API.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Step 2: Inspect the Data Loaded into the Database\\n\",\n",
    "    \"**Goal:** Now let's check our SQL database. Did the data for this fund, using the **exact name** we found in Step 1, actually get saved correctly?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if 'exact_scheme_name' in locals():\\n\",\n",
    "    \"    print(f\\\"Querying the database for: '{exact_scheme_name}'\\\")\\n\",\n",
    "    \"    engine = create_engine(db_config.url)\\n\",\n",
    "    \"    query = f\\\"SELECT * FROM nav_data WHERE scheme_name = '{exact_scheme_name}'\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        db_df = pd.read_sql(query, engine)\\n\",\n",
    "    \"        print(\\\"--- Data Found in SQL Database ---\\\")\\n\",\n",
    "    \"        if not db_df.empty:\\n\",\n",
    "    \"            print(f\\\"Success! Found {len(db_df)} records in the database.\\\")\\n\",\n",
    "    \"            print(\\\"Sample data from DB:\\\")\\n\",\n",
    "    \"            display(db_df.head())\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(\\\"ðŸ›‘ CRITICAL: Fund data was NOT found in the database, even though it exists at the source.\\\")\\n\",\n",
    "    \"            print(\\\"This suggests a problem during the data extraction or loading phase in your pipeline.\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"An error occurred while querying the database: {e}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Skipping database check because fund was not found at the source.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Step 3: Replicate the Analysis Notebook's Loading Process\\n\",\n",
    "    \"**Goal:** Finally, let's replicate exactly what the forecasting notebook does to see why it's getting 0 records.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"--- Simulating the Forecasting Notebook --- \\\")\\n\",\n",
    "    \"engine = create_engine(db_config.url)\\n\",\n",
    "    \"full_query = \\\"SELECT * FROM nav_data\\\"\\n\",\n",
    "    \"full_df_from_db = pd.read_sql(full_query, engine)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# This is the exact filter that was failing\\n\",\n",
    "    \"analysis_df = full_df_from_db[full_df_from_db['scheme_name'] == TARGET_FUND_NAME]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Attempting to filter for: '{TARGET_FUND_NAME}'\\\")\\n\",\n",
    "    \"print(f\\\"Number of records found: {len(analysis_df)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(analysis_df) == 0 and 'exact_scheme_name' in locals() and TARGET_FUND_NAME != exact_scheme_name:\\n\",\n",
    "    \"    print(\\\"\\\\n--- DIAGNOSIS ---\\\")\\n\",\n",
    "    \"    print(\\\"The problem is a name mismatch!\\\")\\n\",\n",
    "    \"    print(f\\\"You are filtering for: '{TARGET_FUND_NAME}'\\\")\\n\",\n",
    "    \"    print(f\\\"But the name in the database is: '{exact_scheme_name}'\\\")\\n\",\n",
    "    \"    print(\\\"Please use the exact name from the database in your analysis notebooks.\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2aacae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_local_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_local_pipeline.py\n",
    "\"\"\"\n",
    "This script runs the local data ingestion pipeline to populate the database.\n",
    "\"\"\"\n",
    "import logging\n",
    "from data_extractor import MFDataExtractor\n",
    "from db_loader import DBLoader\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Runs the data extraction and loading pipeline.\"\"\"\n",
    "    logging.info(\"--- Starting Local Data Pipeline Run ---\")\n",
    "    try:\n",
    "        # 1. Initialize components\n",
    "        extractor = MFDataExtractor()\n",
    "        loader = DBLoader()\n",
    "\n",
    "        # 2. Extraction from the official AMFI source\n",
    "        logging.info(\"Extracting data from AMFI source...\")\n",
    "        nav_df = extractor.get_all_nav_data()\n",
    "\n",
    "        if nav_df.empty:\n",
    "            raise ValueError(\"No data was extracted from AMFI. Halting pipeline.\")\n",
    "\n",
    "        # 3. Loading the data into the database using 'append'\n",
    "        logging.info(f\"Loading {len(nav_df)} records to the database...\")\n",
    "        # This will now correctly append data and build a history\n",
    "        success = loader.load_to_db(nav_df, 'nav_data', if_exists='append')\n",
    "        \n",
    "        if not success:\n",
    "            raise ValueError(\"Data loading failed during database operation.\")\n",
    "        \n",
    "        logging.info(\"--- Data Pipeline Completed Successfully ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
